{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keyword Extraction",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BYm2s600wiB4ddwlq_P7-dWGIFxk_TG0",
      "authorship_tag": "ABX9TyNQhiNw9qMv8+zvwLfMrf0l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samuelagyemang2012/Sentiment-Analysis/blob/master/Keyword_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVegINomwNLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc3c0245-abc1-4e74-cfe8-6cb2f956460e"
      },
      "source": [
        "pip install yake"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yake\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/c4de4fb40639ec674f944d82e5b0be5a5a9162fc8e83e379ab10b83ee1f9/yake-0.4.8-py2.py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=6.0 in /usr/local/lib/python3.7/dist-packages (from yake) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from yake) (1.19.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from yake) (2.5.1)\n",
            "Collecting jellyfish\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/a6/4d039bc827a102f62ce7a7910713e38fdfd7c7a40aa39c72fb14938a1473/jellyfish-0.8.2-cp37-cp37m-manylinux2014_x86_64.whl (90kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.7MB/s \n",
            "\u001b[?25hCollecting segtok\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from yake) (0.8.9)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->yake) (4.4.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok->yake) (2019.12.20)\n",
            "Building wheels for collected packages: segtok\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25031 sha256=c5bb954730e31eeffc5712ab473cb745a15b1dcefa938db3a405391ef38337d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "Successfully built segtok\n",
            "Installing collected packages: jellyfish, segtok, yake\n",
            "Successfully installed jellyfish-0.8.2 segtok-1.5.10 yake-0.4.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtZEfFR80XMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b1746c1-976a-4c37-aa90-44dee2150a7d"
      },
      "source": [
        "pip install rake-nltk"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rake-nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->rake-nltk) (1.15.0)\n",
            "Building wheels for collected packages: rake-nltk\n",
            "  Building wheel for rake-nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rake-nltk: filename=rake_nltk-1.0.4-py2.py3-none-any.whl size=7829 sha256=0acd9c00e10fcb093f27799e9c427382293cb6fa6f14952255d3780f522be538\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/92/fc/271b3709e71a96ffe934b27818946b795ac6b9b8ff8682483f\n",
            "Successfully built rake-nltk\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0brYVaE-wIEL",
        "outputId": "50eb735e-b823-49a2-ce6c-5e0e7b3add60"
      },
      "source": [
        "pip install clean-text"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting clean-text\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/f5/f0db7d0185e26f9a85d425fd19d9bd891ff733f2ec0d1ee06791fd7f13b6/clean_text-0.4.0-py3-none-any.whl\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.9MB/s \n",
            "\u001b[?25hCollecting ftfy<7.0,>=6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=15fcd328daf6616052627510e4cdefc8af59e5bb5ed88b08cc77a737bcfc0eab\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n",
            "Successfully built ftfy\n",
            "Installing collected packages: emoji, ftfy, clean-text\n",
            "Successfully installed clean-text-0.4.0 emoji-1.2.0 ftfy-6.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjEmRJe_DyUu",
        "outputId": "4f0141ef-de11-462c-c04e-033e93cbf63b"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 24.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyaQ14ylRflW",
        "outputId": "1db6682a-8507-4e71-c700-fcc4bd1750a4"
      },
      "source": [
        "pip install langdetect"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-cp37-none-any.whl size=993242 sha256=6631ce5c72de862fdb8289c2832867d918f03798f1602d21d65118ae5172f1fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/18/13/038c34057808931c7ddc6c92d3aa015cf1a498df5a70268996\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ia8sFPX5dXt"
      },
      "source": [
        "#Python dictionary and engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6tJYzRm4MpR",
        "outputId": "a0c6af8f-78eb-43fd-9d81-4dbd2b33fa80"
      },
      "source": [
        "pip install pyenchant"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyenchant\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/f1/162fc6975068098e3327358216b70bbecba1d8004438c3bc8fe9f9378a89/pyenchant-3.2.1-py3-none-any.whl (55kB)\n",
            "\r\u001b[K     |█████▉                          | 10kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 20kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 30kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 40kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 51kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5DlmEiF5SAG",
        "outputId": "185893d2-ce7c-4065-be80-2d3df1d3f7a0"
      },
      "source": [
        "!apt install -qq enchant"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 1,310 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 160815 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47SochZ_qJ3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eef5092-165e-4b48-fba2-24d5a26041be"
      },
      "source": [
        "import enchant\n",
        "import spacy\n",
        "import yake\n",
        "from rake_nltk import Rake\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"stopwords\")\n",
        "import pprint\n",
        "from cleantext import clean\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from langdetect import detect"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj711iPAq4fc"
      },
      "source": [
        "# Import reviews\n",
        "# honey_data_path = \"/content/drive/MyDrive/ML Data/bemyhoneydata.csv\"\n",
        "reviews_data_path = \"/content/drive/MyDrive/ML Data/hotel_reviews.csv\"\n",
        "# redmi6_data_path = \"/content/drive/MyDrive/ML Data/redmi6.csv\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX-IkqL9rVFE",
        "outputId": "e36481f5-b253-405f-92b3-3420bb0fcc8b"
      },
      "source": [
        "# Display reviews\n",
        "df = pd.read_csv(reviews_data_path)\n",
        " \n",
        "print(df.head())\n",
        "# print(\"\")\n",
        "# print(honey_df.head())\n",
        "# print(\"\")\n",
        "# print(redmi_df.head())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  address categories  ... reviews.username reviews.userProvince\n",
            "0  Riviera San Nicol 11/a     Hotels  ...      Russ (kent)                  NaN\n",
            "1  Riviera San Nicol 11/a     Hotels  ...       A Traveler                  NaN\n",
            "2  Riviera San Nicol 11/a     Hotels  ...             Maud                  NaN\n",
            "3  Riviera San Nicol 11/a     Hotels  ...            Julie                  NaN\n",
            "4  Riviera San Nicol 11/a     Hotels  ...         sungchul                  NaN\n",
            "\n",
            "[5 rows x 19 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQLpZkR9uzBD"
      },
      "source": [
        "# Fetch reviews\n",
        "# honey_list = honey_df[\"comments\"].to_list()\n",
        "# reviews_list = reviews_df[\"Review\"].to_list()\n",
        "# redmi_list = redmi_df[\"Comments\"].to_list()\n",
        "\n",
        "# # print(len(reviews_list))\n",
        "# reviews_test = reviews_list[0:2000]\n",
        "# print(reviews_test[3])\n",
        "reviews_subset = df[[\"name\", \"reviews.text\"]]\n",
        "hotel_names = reviews_subset.name.unique()\n",
        "hotel_names = hotel_names.tolist()\n",
        "hotel_names.sort()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAn486q7SK87",
        "outputId": "ce1c73b9-1b95-4db3-82df-aa644d41f78d"
      },
      "source": [
        "hotel = \"Solstice\"\n",
        "\n",
        "hotel_df = df.loc[df['name'] == hotel]\n",
        "hotel_reviews = hotel_df[\"reviews.text\"].tolist()\n",
        "print(len(hotel_reviews))\n",
        "print(hotel_reviews[0])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "114\n",
            "If you are looking for comfort, DO NOT CHOOSE this location. The beds are Concrete SLABS. The Shower pressure was horrific, The drain did not work. The ceiling was failing in and was molded. The Carpet was stained. There was no Parking because of equipment for construction. The window was covered by dirty plastic so there was no view.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yCvJBtv9mTo"
      },
      "source": [
        "# Download Spacy model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFxNtO-_s_cQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac10657-138c-4173-82d4-272614df1481"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nRBrwYKtuUH"
      },
      "source": [
        "# !python -m spacy download en_core_web_lg"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30VSaVt8w2JL"
      },
      "source": [
        "# Load Spacy model and increase max numer if chars to be processed\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.max_length = 2000000"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxc0Y0LH6VS1"
      },
      "source": [
        "# Fetch stopwords\n",
        "en_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        " \n",
        "# Prepocessing for sentences alg1\n",
        "def preprocess(sentences):\n",
        "  data=[]\n",
        "  for x in sentences:\n",
        "    try:\n",
        "      if detect(x) == 'en':\n",
        "        x = x.lower()\n",
        "        x = ' '.join([word for word in x.split(' ') if word not in en_stopwords])\n",
        "        x = x.encode('ascii', 'ignore').decode()\n",
        "        x = re.sub(r'https*\\S+', ' ', x)\n",
        "        x = re.sub(r'@\\S+', ' ', x)\n",
        "        x = re.sub(r'#\\S+', ' ', x)\n",
        "        x = re.sub(r'\\'\\w+', '', x)\n",
        "        x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
        "        x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
        "        x = re.sub(r'\\s{2,}', ' ', x)\n",
        " \n",
        "        data.append(x)\n",
        "    except:\n",
        "      pass\n",
        "  return data\n",
        "\n",
        "# Prepocessing for sentences alg2, uses clean text package\n",
        "def clean_text(sentences):\n",
        "  data=[]\n",
        "  for x in sentences:\n",
        "    \n",
        "    try: \n",
        "      if detect(x) == 'en':\n",
        "        x = x.lower()\n",
        "        x = ' '.join([word for word in x.split(' ') if word not in en_stopwords])\n",
        "        x = clean(x,\n",
        "        fix_unicode=True,               # fix various unicode errors\n",
        "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
        "        lower=True,                     # lowercase text\n",
        "        no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
        "        no_urls=True,                  # replace all URLs with a special token\n",
        "        no_emails=True,                # replace all email addresses with a special token\n",
        "        no_phone_numbers=True,         # replace all phone numbers with a special token\n",
        "        no_numbers=True,               # replace all numbers with a special token\n",
        "        no_digits=True,                # replace all digits with a special token\n",
        "        no_currency_symbols=True,      # replace all currency symbols with a special token\n",
        "        no_punct=True,                 # remove punctuations\n",
        "        replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
        "        replace_with_url=\"<URL>\",\n",
        "        replace_with_email=\"<EMAIL>\",\n",
        "        replace_with_phone_number=\"<PHONE>\",\n",
        "        replace_with_number=\"<NUMBER>\",\n",
        "        replace_with_digit=\"0\",\n",
        "        replace_with_currency_symbol=\"<CUR>\",\n",
        "        lang=\"en\")                       # set to 'de' for German special handling\n",
        "                       \n",
        "        data.append(x)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      pass\n",
        "\n",
        "  return data\n",
        "\n",
        "def pretty_print(my_list):\n",
        "  pp = pprint.PrettyPrinter(indent=4)\n",
        "  pp.pprint(my_list)\n",
        "\n",
        "# Merge all documents into one document\n",
        "def merge_data(my_list):\n",
        "  all_data = \"\"\n",
        "  for d in my_list:\n",
        "    all_data += d+\"\\n\"\n",
        " \n",
        "  return all_data\n",
        "\n",
        "# Keyword extraction with spacy\n",
        "def spacy_key_word_extract(data, max):\n",
        "  # nlp = spacy.load(\"en_core_web_md\")\n",
        "  doc = nlp(data)\n",
        "  print(doc.ents[0:max])\n",
        "\n",
        "# Keyword extraction with Yake \n",
        "def yake_key_word_extract(data, lang, max_ngram_size, deduplication_threshold, max_words):\n",
        "  # extractor = yake.KeywordExtractor()\n",
        "  # max_ngram_size = 3\n",
        "  # deduplication_threshold = 0.9\n",
        "  # numOfKeywords = 20\n",
        "  kws = []\n",
        "  custom_kw_extractor = yake.KeywordExtractor(lan=lang, n=max_ngram_size, dedupLim=deduplication_threshold, top=max_words, features=None)\n",
        "  keywords = custom_kw_extractor.extract_keywords(data)\n",
        "  for kw in keywords:\n",
        "    kws.append(kw)\n",
        "    print(kw)\n",
        "  return kws\n",
        "\n",
        "# Keyword extraction with Yake default\n",
        "def yake_key_word_extract_raw(data,max_words):\n",
        "  extractor = yake.KeywordExtractor(top=max_words)\n",
        "  keywords = extractor.extract_keywords(data)\n",
        "  for kw in keywords:\n",
        "    print(kw)\n",
        "\n",
        "# Keyword extraction with Rake\n",
        "def rake_key_word_extract(data,max):\n",
        "  rake_model = Rake(min_length=2, max_length=4)\n",
        "  rake_model.extract_keywords_from_text(data)\n",
        "  keyword_extracted = rake_model.get_ranked_phrases()[0:max]\n",
        "  for kw in keyword_extracted:\n",
        "    print(kw)\n",
        "\n",
        "def sort_tuple(tup, reverse):\n",
        "\n",
        "\t#reverse = None (Sorts in Ascending order)\n",
        "\t# key is set to sort using second element of\n",
        "\t# sublist lambda has been used\n",
        "\ttup.sort(key = lambda x: x[1], reverse=reverse)\n",
        "\treturn tup\n",
        "\n",
        "# Extract all nouns from a document\n",
        "def extract_nouns(doc, zlp):\n",
        "  doc = zlp(doc)\n",
        "  all_nouns = \"\"\n",
        "  # Fetch all nouns in document\n",
        "  for token in doc:\n",
        "    # if token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\":\n",
        "    if token.pos_ == \"NOUN\":\n",
        "      nn = token.text\n",
        "      all_nouns += nn+\" \"\n",
        "      \n",
        "  return all_nouns"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ5GStKn6fbK"
      },
      "source": [
        "data = hotel_reviews\n",
        " \n",
        "# Data cleaning\n",
        "cleaned_data = clean_text(data)\n",
        "cleaned_data1 = preprocess(data)\n",
        "\n",
        "# Merge data into one document\n",
        "merged_data = merge_data(cleaned_data)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR2ewp0hvja0",
        "outputId": "f63cad25-6a68-4d5f-d2a8-dac6bf48dbcc"
      },
      "source": [
        "print(len(cleaned_data))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKgPufO3r0mb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6089539-8683-4ca3-dd3d-362275dbc471"
      },
      "source": [
        "# Comparing text cleaning algs, alg2 doesnt get rid of emojis..\n",
        "for i in range(len(cleaned_data[0:5])):\n",
        "  print(cleaned_data[i])\n",
        "  print(cleaned_data1[i])\n",
        "  print(\"\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looking comfort choose location beds concrete slabs shower pressure horrific drain work ceiling failing molded carpet stained parking equipment construction window covered dirty plastic view\n",
            "looking comfort choose location beds concrete slabs shower pressure horrific drain work ceiling failing molded carpet stained parking equipment construction window covered dirty plastic view \n",
            "\n",
            "great location far highway close tons restaurants far millcreek mall shopping opportunities good value price would stay ever find erie pa again\n",
            "great location far highway close tons restaurants far millcreek mall shopping opportunities good value price would stay ever find erie pa again \n",
            "\n",
            "reading reviews little scared stay there booked country inn suites brand cause stayed many times past knew first week september hotel fine rooms great bed comfortable\n",
            "reading reviews little scared stay there booked country inn suites brand cause stayed many times past knew first week september hotel fine rooms great bed comfortable \n",
            "\n",
            "name hotel changed solstice great location close presque isle beaches waldameer park staff excellent loved pool pool room warm water <number> degrees tables chairs lounge stacks towels use breakfast great donuts muffins fruit waffle machine eggs and\n",
            "name hotel changed solstice great location close presque isle beaches waldameer park staff excellent loved pool pool room warm water degrees tables chairs lounge stacks towels use breakfast great donuts muffins fruit waffle machine eggs and \n",
            "\n",
            "country inn solstice inn horrible air conditioning room work <number> degrees outside <number> inside one available look gave us fan stick window room next busy road it\n",
            "country inn solstice inn horrible air conditioning room work degrees outside inside one available look gave us fan stick window room next busy road it \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hBYCjLMyQLt",
        "outputId": "691667d4-10f8-4816-a0df-8fdda91d36d3"
      },
      "source": [
        "# Get top 10 keywords\n",
        "kw=10\n",
        "print(\"Spacy\")\n",
        "spacy_key_word_extract(merged_data, kw)\n",
        "print(\"\")\n",
        "print(\"Yake\")\n",
        "yake_key_word_extract(merged_data, \"en\", 1, 0.5, kw)\n",
        "print(\"\")\n",
        "print(\"Yake Raw\")\n",
        "yake_key_word_extract_raw(merged_data,kw)\n",
        "print(\"\")\n",
        "print(\"Rake\")\n",
        "rake_key_word_extract(merged_data,kw)\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spacy\n",
            "(concrete slabs shower, reviews, first week, september, beaches waldameer, > degrees, > degrees, one night, hotter, us)\n",
            "\n",
            "Yake\n",
            "('hotel', 0.0006492541160390149)\n",
            "('room', 0.0007017390957222177)\n",
            "('breakfast', 0.0009856554508049383)\n",
            "('stay', 0.0010790557204821728)\n",
            "('pool', 0.0011826130594175452)\n",
            "('good', 0.0016264882708041833)\n",
            "('number', 0.0017592999806748803)\n",
            "('inn', 0.0019385368397029452)\n",
            "('great', 0.0021816318905087157)\n",
            "('country', 0.002302203183798257)\n",
            "\n",
            "Yake Raw\n",
            "('country inn suites', 4.203073169214891e-06)\n",
            "('booked country inn', 2.327169433893406e-05)\n",
            "('inn suites solstice', 3.824990475967703e-05)\n",
            "('pool staff friendly', 4.9368524668167553e-05)\n",
            "('front desk staff', 5.0454019404101544e-05)\n",
            "('desk staff friendly', 5.1759435846601916e-05)\n",
            "('inn solstice inn', 5.872172466521838e-05)\n",
            "('stay country inn', 5.920845961037018e-05)\n",
            "('indoor pool staff', 6.067767442071832e-05)\n",
            "('hot tub pool', 6.655966230485017e-05)\n",
            "\n",
            "Rake\n",
            "one look color water\n",
            "lots hotels erie beware\n",
            "line hotelscom halfway lights\n",
            "faucet shaky breakfast wasmeh\n",
            "bagles couple pieces bread\n",
            "would never stay hotel\n",
            "number >< number\n",
            "made reservation approximately\n",
            "frustrating reservation booked\n",
            "didnt seem moving\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXqw_OwAoAr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38a73530-e6d2-40e2-86a8-39b74b789a81"
      },
      "source": [
        "# Prepare document for POS tagging using spacy\n",
        "sentence = merged_data\n",
        "\n",
        "# Extract all nouns from document\n",
        "all_nouns = extract_nouns(sentence, nlp)\n",
        "\n",
        "# Top n keywords extracted from all nouns in document\n",
        "n = 10\n",
        "\n",
        "ykws1 = yake_key_word_extract(all_nouns, \"en\", 1, 0.3, n)\n",
        "print(\"\")\n",
        "\n",
        "ykws1 = yake_key_word_extract(all_nouns, \"en\", 1, 0.5, n)\n",
        "print(\"\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('hotel', 0.0006318162711755036)\n",
            "('room', 0.0006347707161589102)\n",
            "('breakfast', 0.001227975437530355)\n",
            "('staff', 0.0015736588671568124)\n",
            "('number', 0.0020932579188859247)\n",
            "('night', 0.0034260464455620355)\n",
            "('desk', 0.003784865317701123)\n",
            "('work', 0.004049690033186636)\n",
            "('place', 0.00509432900139619)\n",
            "('country', 0.00516379038751804)\n",
            "\n",
            "('hotel', 0.0006318162711755036)\n",
            "('room', 0.0006347707161589102)\n",
            "('breakfast', 0.001227975437530355)\n",
            "('staff', 0.0015736588671568124)\n",
            "('pool', 0.001621333701037581)\n",
            "('number', 0.0020932579188859247)\n",
            "('night', 0.0034260464455620355)\n",
            "('desk', 0.003784865317701123)\n",
            "('bed', 0.003934137043228823)\n",
            "('work', 0.004049690033186636)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q6yBImbG0lm"
      },
      "source": [
        "# Enchant Dictionanary Word Suggestion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMs5oTjJ1iFX",
        "outputId": "16d73bfb-daa7-419d-d8ea-0ea934910830"
      },
      "source": [
        "d = enchant.Dict(\"en_US\")\n",
        "print(d.check(\"moisturizer\"))\n",
        "d.suggest(\"breakfast.Spa\")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['breakfast. Spa', 'breakfast']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    }
  ]
}